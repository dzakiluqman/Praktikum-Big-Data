{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPouh3EugMN3BLzSQs+zpWk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1NoCtgw_52w","executionInfo":{"status":"ok","timestamp":1765399509267,"user_tz":-420,"elapsed":33284,"user":{"displayName":"Dzakii Luqman Faid","userId":"12690720904230195207"}},"outputId":"4a0f5d42-0f49-4c76-ed76-3f824b0ab63c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficients: [0.9999999999999992]\n","Intercept: 15.000000000000009\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml.feature import VectorAssembler\n","\n","# Initialize Spark Session\n","spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n","\n","# Load sample data\n","data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n","columns = ['ID', 'Feature', 'Target']\n","df = spark.createDataFrame(data, columns)\n","\n","# Prepare data for modeling\n","assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n","df_transformed = assembler.transform(df)\n","\n","# Train a linear regression model\n","lr = LinearRegression(featuresCol='Features', labelCol='Target')\n","model = lr.fit(df_transformed)\n","\n","# Print model coefficients\n","print(f'Coefficients: {model.coefficients}')\n","print(f'Intercept: {model.intercept}')"]},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.linalg import Vectors\n","\n","# Example dataset with Vector type\n","data = [\n","    (1, Vectors.dense([2.0, 3.0]), 0),\n","    (2, Vectors.dense([1.0, 5.0]), 1),\n","    (3, Vectors.dense([2.5, 4.5]), 1),\n","    (4, Vectors.dense([3.0, 6.0]), 0)\n","]\n","\n","columns = ['ID', 'Features', 'Label']\n","df = spark.createDataFrame(data, columns)\n","\n","# Train logistic regression model\n","lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n","model = lr.fit(df)\n","\n","print(f\"Coefficients: {model.coefficients}\")\n","print(f\"Intercept: {model.intercept}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYb4gguCAkr9","executionInfo":{"status":"ok","timestamp":1765400083872,"user_tz":-420,"elapsed":9628,"user":{"displayName":"Dzakii Luqman Faid","userId":"12690720904230195207"}},"outputId":"8ab70d48-9af4-4edb-9deb-6f8261fd1194"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Coefficients: [-12.26205792372122,4.087352264669246]\n","Intercept: 11.568912721182315\n"]}]},{"cell_type":"code","source":["from pyspark.ml.clustering import KMeans\n","from pyspark.ml.linalg import Vectors\n","\n","# Example dataset with Vector type\n","data = [\n","    (1, Vectors.dense([1.0, 1.0])),\n","    (2, Vectors.dense([5.0, 5.0])),\n","    (3, Vectors.dense([10.0, 10.0])),\n","    (4, Vectors.dense([15.0, 15.0]))\n","]\n","\n","columns = ['ID', 'Features']\n","df = spark.createDataFrame(data, columns)\n","\n","# Train KMeans clustering model\n","kmeans = KMeans(featuresCol='Features', k=2)\n","model = kmeans.fit(df)\n","\n","# Show cluster centers\n","centers = model.clusterCenters()\n","print(\"Cluster Centers:\")\n","for c in centers:\n","    print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ng7ob1wOAmbd","executionInfo":{"status":"ok","timestamp":1765400148528,"user_tz":-420,"elapsed":5396,"user":{"displayName":"Dzakii Luqman Faid","userId":"12690720904230195207"}},"outputId":"944ecefd-8e4c-4ff8-8d73-396cc43aca92"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cluster Centers:\n","[5.33333333 5.33333333]\n","[15. 15.]\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import when, col\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","\n","# Load dataset\n","df = spark.read.csv(\"job_market.csv\", header=True, inferSchema=True)\n","\n","# Ambil kolom numerik saja\n","df = df.select(\"salary_min\", \"salary_max\", \"experience_required\")\n","\n","# Drop row kosong\n","df = df.dropna()\n","\n","# Definisikan label: high_salary\n","median_salary = df.approxQuantile(\"salary_max\", [0.5], 0.1)[0]\n","\n","df = df.withColumn(\n","    \"label\",\n","    when(col(\"salary_max\") >= median_salary, 1).otherwise(0)\n",")\n","\n","# Vector assembler\n","assembler = VectorAssembler(\n","    inputCols=[\"salary_min\", \"salary_max\", \"experience_required\"],\n","    outputCol=\"features\"\n",")\n","\n","final_data = assembler.transform(df).select(\"features\", \"label\")\n","\n","final_data.show(5, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmepwcqVDRY9","executionInfo":{"status":"ok","timestamp":1765400657291,"user_tz":-420,"elapsed":1460,"user":{"displayName":"Dzakii Luqman Faid","userId":"12690720904230195207"}},"outputId":"217c0181-886d-47b3-d442-37700620ee85"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------+-----+\n","|features               |label|\n","+-----------------------+-----+\n","|[151082.0,291345.0,4.0]|1    |\n","|[156891.0,280075.0,3.0]|1    |\n","|[152134.0,280310.0,4.0]|1    |\n","|[151918.0,253988.0,7.0]|1    |\n","|[148141.0,252584.0,9.0]|1    |\n","+-----------------------+-----+\n","only showing top 5 rows\n"]}]},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n","\n","# Split data\n","train, test = final_data.randomSplit([0.8, 0.2], seed=42)\n","\n","# Model\n","lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n","\n","lr_model = lr.fit(train)\n","\n","pred = lr_model.transform(test)\n","\n","# Evaluasi\n","acc = MulticlassClassificationEvaluator(metricName=\"accuracy\").evaluate(pred)\n","f1  = MulticlassClassificationEvaluator(metricName=\"f1\").evaluate(pred)\n","auc = BinaryClassificationEvaluator(metricName=\"areaUnderROC\").evaluate(pred)\n","\n","print(\"=== PERFORMANCE ===\")\n","print(\"Accuracy :\", acc)\n","print(\"F1 Score :\", f1)\n","print(\"AUC ROC  :\", auc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JeZcSNwYFGJc","executionInfo":{"status":"ok","timestamp":1765400681173,"user_tz":-420,"elapsed":5817,"user":{"displayName":"Dzakii Luqman Faid","userId":"12690720904230195207"}},"outputId":"8bc3ff50-7d1b-4771-bea3-a5e57b159d0d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["=== PERFORMANCE ===\n","Accuracy : 1.0\n","F1 Score : 1.0\n","AUC ROC  : 1.0\n"]}]},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","paramGrid = (ParamGridBuilder()\n","             .addGrid(lr.regParam, [0.01, 0.1])\n","             .addGrid(lr.elasticNetParam, [0.0, 0.5])\n","             .build())\n","\n","cv = CrossValidator(\n","    estimator=lr,\n","    estimatorParamMaps=paramGrid,\n","    evaluator=BinaryClassificationEvaluator(metricName=\"areaUnderROC\"),\n","    numFolds=3\n",")\n","\n","cv_model = cv.fit(train)\n","cv_pred = cv_model.transform(test)\n","\n","print(\"Best regParam:\", cv_model.bestModel._java_obj.getRegParam())\n","print(\"Best elasticNetParam:\", cv_model.bestModel._java_obj.getElasticNetParam())\n","\n","final_auc = BinaryClassificationEvaluator(metricName=\"areaUnderROC\").evaluate(cv_pred)\n","print(\"AUC After CV:\", final_auc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EG1cb2RNFKEj","executionInfo":{"status":"ok","timestamp":1765400708953,"user_tz":-420,"elapsed":24059,"user":{"displayName":"Dzakii Luqman Faid","userId":"12690720904230195207"}},"outputId":"bf26f4ac-b0e6-4af5-fc69-2b7c0179a751"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Best regParam: 0.01\n","Best elasticNetParam: 0.5\n","AUC After CV: 1.0\n"]}]}]}